{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src='./text_images/nvidia.png' width=\"200\" height=\"450\">\n",
    "        </td>\n",
    "        <td> & </td>\n",
    "        <td>\n",
    "            <img src='./text_images/udacity.png' width=\"350\" height=\"450\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"font-family:Georgia;\n",
    "              font-size:1.6vw;\n",
    "              color:#017A9B;\n",
    "              font-style:bold;\n",
    "              text-align:left;\n",
    "              background:url('./text_images/4.jpeg') no-repeat center;\n",
    "              background-size:cover)\">\n",
    "              \n",
    "     <br><br>\n",
    "     DRL for Optimal Execution of Portfolio Transactions     \n",
    "     <br><br>\n",
    "    \n",
    "     \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We begin with a brief review of reinforcement learning and actor-critic methods.  Then, you will use an actor-critic method to generate optimal trading strategies that maximize profit when liquidating a block of shares. \n",
    "\n",
    "In reinforcement learning, an agent makes observations and takes actions within an environment, and in return it receives rewards.  Its objective is to learn to act in a way that will maximize its expected long-term rewards. \n",
    "\n",
    "<img src=\"./text_images/RL.png\" width=\"500\" height=\"900\">\n",
    "\n",
    "There are several types of RL algorithms, and they can be divided into three groups:  \n",
    "- critic-only (_or value-based_),\n",
    "- actor-only (_or policy-based_), and\n",
    "- actor-critic methods.\n",
    "\n",
    "The words \"actor\" and \"critic\" are synonyms for the policy and value function, respectively. \n",
    "\n",
    "Q-learning algorithms are considered as **critic-only** methods.  In 2015, [DeepMind published a paper](https://arxiv.org/abs/1312.5602) to show the world how using deep Q-Networks can be used to approximate the value function for complex problems. \n",
    "\n",
    "**Actor-only** methods typically work with a parameterized family of policies over which optimization procedures can be used directly.  To learn more about actor-only methods, you're encouraged to peruse [this paper](http://ieeexplore.ieee.org/document/6392457/).\n",
    "\n",
    "**Actor-critic** methods combine the advantages of actor-only and critic-only methods. While the actor brings the advantage of computing continuous actions without the need for optimization procedures on a value function, the critic’s merit is that it supplies the actor with knowledge of the performance.  Actor-critic methods usually have good convergence properties, in contrast to critic-only methods.  The **Deep Deterministic Policy Gradients (DDPG)** algorithm is one example of an actor-critic method.\n",
    "\n",
    "<img src=\"./text_images/Actor-Critic.png\" width=\"500\" height=\"700\">\n",
    "\n",
    "In this lab, we will use DDPG to determine the optimal execution of portfolio transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Optimal Execution as a Reinforcement Learning Problem\n",
    "---\n",
    "\n",
    "### States\n",
    "We will use the following features to define the state at time $t_k$:\n",
    "\n",
    "$$\n",
    "[r_{k-5},\\, r_{k-4},\\, r_{k-3},\\, r_{k-2},\\, r_{k-1},\\, r_{k},\\, t_{k},\\, i_{k}]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $r_{k} := \\log(\\frac{\\tilde{S}_k}{\\tilde{S}_{k-1}})$ is the log-return at time $t_k$\n",
    "- $i_{k} := X - \\sum_{j=1}^{k}n_j$ is the remaining inventory at time $t_k$ \n",
    "\n",
    "_In a real world setting, if there is enough data, market variables such as the current trading rate, current [spread](https://www.investopedia.com/trading/basics-of-the-bid-ask-spread/), and [limit order book](https://www.investopedia.com/terms/l/limitorderbook.asp) density can be added to the state vector. You may also consider limit order book imbalance within ϵ basis points from the mid-spread, buy transaction volume, and sell transaction volume. These variables will likely need to be normalized using a long-term average on a per stock basis._\n",
    "\n",
    "### Actions\n",
    "The action $a_k$ at time $t_{k}$ is defined in terms of the average selling rate between times $t_{k}$ and $t_{k+1}$.  When we express the average selling rate is as a multiple of the constant selling rate $\\frac{X}{N}$:\n",
    "\n",
    "$$\n",
    "c_k \\times \\frac{X}{N}\n",
    "$$\n",
    "\n",
    "where $c_k$ is a scalar.  Note that this is equal to $\\frac{n_k}{\\tau}$. yeah need to fix this.\n",
    "\n",
    "### Rewards\n",
    "Defining the rewards is trickier than defining states and actions, since the original problem is a minimization problem. One option is to use difference of two consecutive utility functions of Almgren-Chriss calculated in time. After each time interval, we compute Chriss-Almgren model for the remaining time and inventory while holding parameter λ constant. Denoting optimal trading trajectory computed at time t as $x^*_t$, we define reward as: \n",
    "\n",
    "$$\n",
    "R_{t} = {{U_t(x^*_t) - U_{t+1}(x^*_{t+1})}\\over{U_t(x^*_t)}}\n",
    "$$\n",
    "\n",
    "We normalized the difference to train the actor and critic models easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode [100/100]\tAverage Shortfall: $1,941,808.44\n",
      "\n",
      "Average Implementation Shortfall: $1,941,808.44 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import syntheticChrissAlmgren as sca\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# Create simulation environment\n",
    "env = sca.MarketEnvironment()\n",
    "\n",
    "# Initialize Feed-forward DNNs for Actor and Critic models. \n",
    "agent = Agent(state_size=env.observation_space_dimension(), action_size=env.action_space_dimension(), random_seed=0)\n",
    "\n",
    "# Set the liquidation time\n",
    "lqt = 60\n",
    "\n",
    "# Set the number of trades\n",
    "n_trades = 60\n",
    "\n",
    "# Set trader's risk aversion\n",
    "tr = 1e-6\n",
    "\n",
    "# Set the number of episodes to run the simulation\n",
    "episodes = 100\n",
    "\n",
    "shortfall_hist = np.array([])\n",
    "shortfall_deque = deque(maxlen=100)\n",
    "reward_hist = np.array([])\n",
    "\n",
    "for episode in range(episodes): \n",
    "    # Reset the enviroment\n",
    "    cur_state = env.reset(seed = episode, liquid_time = lqt, num_trades = n_trades, lamb = tr)\n",
    "\n",
    "    # set the environment to make transactions\n",
    "    env.start_transactions()\n",
    "\n",
    "    for i in range(n_trades + 1):\n",
    "      \n",
    "        # Predict the best action for the current state. \n",
    "        action = agent.act(cur_state, add_noise = False)\n",
    "        \n",
    "        # Action is performed and new state, reward, info are received. \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # current state, action, reward, new state are stored in the experience replay\n",
    "        agent.step(cur_state, action, reward, new_state, done)\n",
    "        \n",
    "        # roll over new state\n",
    "        cur_state = new_state\n",
    "        \n",
    "        reward_hist = np.append(reward_hist, reward)\n",
    "\n",
    "        if info.done:\n",
    "            shortfall_hist = np.append(shortfall_hist, info.implementation_shortfall)\n",
    "            shortfall_deque.append(info.implementation_shortfall)\n",
    "            break\n",
    "        \n",
    "    if (episode + 1) % 100 == 0: # print average shortfall over last 100 episodes\n",
    "        print('\\rEpisode [{}/{}]\\tAverage Shortfall: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque)))\n",
    "        \n",
    "\n",
    "print('\\nAverage Implementation Shortfall: ${:,.2f} \\n'.format(np.mean(shortfall_hist)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
